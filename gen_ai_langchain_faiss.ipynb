{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c643e6f3-ebe1-4460-bab6-2c635c0a0bd1",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be3380-2ae1-4706-8c62-261c98b0a1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Build a small Retrieval Augmented Generation application running on PDF files</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9fd47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Services used:**\n",
    "\n",
    "- **LLM:** Oracle GenAI - Cohere Command model\n",
    "- **Integration:** LangChain\n",
    "- **Vector database:** FAISS (in-memory vector database)\n",
    "- **UI:** Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005cd5f-3052-45ad-a81c-27d56db38173",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8dc79-467e-4297-a624-6802523b5395",
   "metadata": {},
   "source": [
    "> ## **Please follow the below steps carefully**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40982140-292c-4af8-84c6-081930a18bdc",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "==============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b7649-8f2c-4c63-8a8a-dc70c169d1c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 1 - Install a Conda environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c5dcb-819b-4ea0-b48d-101eed9357a6",
   "metadata": {},
   "source": [
    "Please follow the below steps in order.\n",
    "\n",
    "1. In the top left corner, click on **\"Launcher\"**. When you don't see the \"Launcher\" tab, click on the blue + button.\n",
    "2. Scroll down a little and click on **\"Terminal\"**. This will open a new terminal.\n",
    "3. **Copy the below snippet**, paste in the terminal, and hit \"Enter\" on your keyboard. This will install the TensorFlow 2.8 for GPU on Python 3.8 Conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bad97d-2b0c-4d95-981d-cadeab74f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Snippet:\n",
    "odsc conda install -s tensorflow28_p38_gpu_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ed0a5-eb5b-4c9e-9617-8f52bef7c275",
   "metadata": {},
   "source": [
    "- 4. Please wait untill the terminal has finished installing the Conda environment. **This may take 2 - 4 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756725f9-5ba4-4955-b63e-1355f58c0061",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff1b60-d299-4512-91e9-580cb1950fd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 2 - Install additional Python packages in the Conda environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135c7bb-527b-4990-99eb-3696cfbb4735",
   "metadata": {},
   "source": [
    "Please follow the below steps in order.\n",
    "\n",
    "- 1. Make sure the terminal has finished installing the conda environment! \n",
    "- 2. In the top right corner, open the kernel by clicking on **\"Python 3 (ipykernel)\"**. This opens a list, select the conda environment you just installed, which is **\"Python[conda env:tensorflow28_p38_gpu_v1\"**\n",
    "\n",
    "When the tensorflow28_p38_gpu_v1 conda is not visible yet, please refresh the page or check the terminal that the conda has properly installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5d2c8-e010-414e-8f3a-f3b9002d7e3e",
   "metadata": {},
   "source": [
    "- 3. When you select the tensorflow28_p38_gpu_v1 in the top right kernel, run the below cell. This will install additional Python packages in the conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba47baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell when you have selected the tensorflow28_p38_gpu_v1 in the top right kernel\n",
    "!pip install accelerate tiktoken openai torch accelerate safetensors sentence-transformers faiss-gpu bitsandbytes pypdf typing-extensions PyPDF2 \n",
    "!pip install tokenizers --upgrade\n",
    "!pip install transformers -U\n",
    "!pip install langchain -U\n",
    "!pip install langchain-community --upgrade\n",
    "!pip install gradio --upgrade\n",
    "!pip install sqlalchemy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376b14-75f2-4fb6-8b77-2b8c296bae8d",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f14de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Step 3 - Run the below cells and click on the public URL button to open the application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb961d0e-705c-40ae-b3d5-9a612cff036a",
   "metadata": {},
   "source": [
    "- 1. Run the below cell, this will display an overview of the steps the application takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac9bc9-493f-47fe-8dfd-a816e8421ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image = mpimg.imread(\"overview.png\")\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0671cb9-9467-4f28-8b14-d7d70a1d5ad5",
   "metadata": {},
   "source": [
    "- 2. Run the below cell. Scroll down to the end of the cell and click on the URL after **Running on public URL**\n",
    "- 3. To cancel the application, click on the square button (interrupt Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a916f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import langchain_community\n",
    "import langchain\n",
    "import oci\n",
    "import gradio as gr\n",
    "import torch\n",
    "import PyPDF2 # pdf reader\n",
    "import time\n",
    "import oci\n",
    "import ads\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain.chains import RetrievalQA \n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.document_loaders import PyPDFDirectoryLoader \n",
    "from transformers import AutoTokenizer\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.llms import OCIGenAI\n",
    "import transformers\n",
    "import tokenizers\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "## set the compartment id\n",
    "compartment_ocid=\"ocid1.compartment.oc1..aaaaaaaafb4m3phzplh5pxmkwpwmgyavgrjz2ta5xmegdzd26nk3xjmenadq\"                  ## Do not change or add your own compartment OCID\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Settings\n",
    "##########################################################################################################################################\n",
    "\n",
    "max_return_from_vector = 4\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Embeddings Model\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "# Using HuggingFaceEmbeddings with the chosen embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs = {\"device\": \"cpu\"})                                                                         \n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Load Oracle GenAI service (Cohere LLM) \n",
    "##########################################################################################################################################\n",
    "\n",
    "def load_llm():\n",
    "    \n",
    "    print(\"Start load GenAI\")\n",
    "    \n",
    "    llm = OCIGenAI(\n",
    "    model_id=\"cohere.command\",\n",
    "    service_endpoint=\"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\",\n",
    "    compartment_id=compartment_ocid,\n",
    "    auth_type=\"RESOURCE_PRINCIPAL\",          #note. We are using resource principal for authentication. Dynamic group and policy should be in order\n",
    "    model_kwargs={\"temperature\": 0.7, \"top_p\": 0.75, \"max_tokens\": 200},\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Create history\n",
    "##########################################################################################################################################\n",
    "\n",
    "def add_text(history, text):\n",
    "\n",
    "    print(\"Start add text\")\n",
    "    if not text:\n",
    "        raise gr.Error('Enter text')\n",
    "    history = history + [(text, '')]\n",
    "    return history\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Upload files\n",
    "##########################################################################################################################################\n",
    "\n",
    "def upload_file(files):\n",
    "    print(type(files))\n",
    "    print(\"done with upload_files\")\n",
    "    \n",
    "    files = files[0].name\n",
    "    print(files)\n",
    "\n",
    "    return files\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Process files\n",
    "##########################################################################################################################################\n",
    "\n",
    "def process_file(files):\n",
    "\n",
    "    print(\"start process_files\")\n",
    "    \"\"\"Function reads each loaded file, and extracts text from each of their pages\n",
    "    The extracted text is store in the 'text variable which is the passed to the splitter\n",
    "    to make smaller chunks necessary for easier information retrieval and adhere to max-tokens(4096)\"\"\"\n",
    "\n",
    "    \n",
    "    ### Read PDF pages and extract text\n",
    "    pdf_text = \"\"\n",
    "    for file in files:\n",
    "        pdf = PyPDF2.PdfReader(file.name)\n",
    "        print(pdf)\n",
    "        for page in pdf.pages:\n",
    "            pdf_text += page.extract_text()\n",
    "\n",
    "\n",
    "    # split PDF files/text smaller chunks\n",
    "    print(\"Start chuncking\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=200)\n",
    "    splits = text_splitter.create_documents([pdf_text])\n",
    "\n",
    "    # create a FAISS vector store db. Create embeddings and adding to faiss\n",
    "    print(\"Store embeddings in FAISS\")\n",
    "    vectorstore_db = FAISS.from_documents(splits, embeddings)\n",
    "    \n",
    "\n",
    "    #create a custom prompt\n",
    "    custom_prompt_template = \"\"\"You have been given the following documents to answer the user's question.\n",
    "    If you do not have information from the files given to answer the questions just say I don't have information from the given files to answer. Do not try to make up an answer. Only use maximum 2 sentences to answer the question.\n",
    "    Context: {context}\n",
    "    History: {history}\n",
    "    Question: {question}\n",
    "\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Full prompt template, including history, context\n",
    "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=[\"question\", \"context\", \"history\"])\n",
    "\n",
    "    # set QA chain with memory\n",
    "    qa_chain_with_memory = RetrievalQA.from_chain_type(llm=load_llm(),\n",
    "                                                       chain_type='stuff',\n",
    "                                                       return_source_documents=True,\n",
    "                                                       retriever=vectorstore_db.as_retriever(search_kwargs={\"k\": max_return_from_vector}),\n",
    "                                                                           chain_type_kwargs={\"verbose\": False,\n",
    "                                                                                              \"prompt\": prompt,\n",
    "                                                                                              \"memory\": ConversationBufferMemory(\n",
    "                                                                                                          input_key=\"question\",\n",
    "                                                                                                          memory_key=\"history\",\n",
    "                                                                                                          return_messages=True) })\n",
    "   \n",
    "    print(\"returning qa_chain_with_memory\")\n",
    "    \n",
    "    return qa_chain_with_memory\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Main\n",
    "##########################################################################################################################################\n",
    "\n",
    "def generate_bot_response(history,query, btn):\n",
    "    \n",
    "    print(\"Start generate_bot_response\")\n",
    "    \n",
    "    #query append\n",
    "    query = query + \", please use only 1 sentence in your answer\"\n",
    "    \n",
    "    qa_chain_with_memory = process_file(btn) # run the qa chain with files from upload\n",
    "    bot_response = qa_chain_with_memory({\"query\": query})\n",
    "    \n",
    "    print(\"--\" *50)\n",
    "    print(\"Bot response is\")\n",
    "    print(bot_response)\n",
    "    print(\"--\" *50)\n",
    " \n",
    "    for char in bot_response['result']:\n",
    "        history[-1][-1] += char\n",
    "        time.sleep(0.05)\n",
    "        yield history,''\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "########################################################################################################################################## Gradio\n",
    "##########################################################################################################################################\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    css=\".contain { display: flex !important; flex-direction: column !important; }\"\n",
    "    \"#component-0, #component-3, #component-10, #component-8  { height: 100% !important; }\"\n",
    "    \"#chatbot { flex-grow: 1 !important; overflow: auto !important;}\"\n",
    "    \"#col { height: 100vh !important; }\"\n",
    "    \n",
    "        \n",
    "    \n",
    "    with gr.Row():\n",
    "            with gr.Row():\n",
    "              # Chatbot interface\n",
    "              chatbot = gr.Chatbot(label=\"Oracle GenAI\",\n",
    "                                   value=[],\n",
    "                                   elem_id='chatbot',\n",
    "                                   render=True,\n",
    "                                    bubble_full_width=False)\n",
    "                \n",
    "                \n",
    "            with gr.Column():\n",
    "                # PDF upload button\n",
    "                btn = gr.UploadButton(\"📁 Upload a PDF\",\n",
    "                                      file_types=[\".pdf\"],\n",
    "                                      file_count=\"multiple\")\n",
    "                with gr.Row():\n",
    "                  # Uploaded PDFs window\n",
    "                  files = gr.File(label=\"Your PDFs\")\n",
    "\n",
    "            \n",
    "\n",
    "    with gr.Column():\n",
    "        with gr.Column():\n",
    "          # Ask question input field\n",
    "          txt = gr.Text(show_label=False, placeholder=\"Enter question\")\n",
    "\n",
    "        with gr.Column():\n",
    "          # button to submit question to the bot\n",
    "          submit_btn = gr.Button('Ask')\n",
    "    \n",
    "\n",
    "    # Event handler for uploading a PDF\n",
    "    btn.upload(fn=upload_file, inputs=[btn], outputs=[files])\n",
    "\n",
    "    # Event handler for submitting text question and generating response\n",
    "    submit_btn.click(\n",
    "        fn= add_text,\n",
    "        inputs=[chatbot, txt],\n",
    "        outputs=[chatbot],\n",
    "        queue=True\n",
    "        ).success(\n",
    "          fn=generate_bot_response,\n",
    "          inputs=[chatbot, txt, btn],\n",
    "          outputs=[chatbot, txt]\n",
    "        ).success(\n",
    "          fn=upload_file,\n",
    "          inputs=[btn],\n",
    "          outputs=[files]\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue()\n",
    "    demo.launch(share=True, debug=True) # launch app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bd118-6db7-4d8e-92fb-ba925d4d4cc1",
   "metadata": {},
   "source": [
    "## **Example conversation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9c829-0d56-4f44-bc20-92ee3ce42019",
   "metadata": {},
   "source": [
    "Why should I travel to Groningen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3e893-e3ee-42c9-9570-c160abc33b56",
   "metadata": {
    "tags": []
   },
   "source": [
    "Yes please. What are new innovations in Groningen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc8ced-cd18-403a-a675-10c77efb8265",
   "metadata": {},
   "source": [
    "Yes. What does advancing hydrogen vehicles mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1adfd8-f75b-4cc9-8a36-172fa957336b",
   "metadata": {},
   "source": [
    "No, thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe71e17-b05a-4b11-91ed-9e6732a5f388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08990f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
